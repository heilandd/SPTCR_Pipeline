#!/usr/bin/env python
import argparse
from random import sample
parser = argparse.ArgumentParser(description='Script to associate TRA & TRB Receptor with MADHYPE Algorithm in spatial context from UMI Correct TCR Table generated by SPTCR-seq Pipeline')

parser.add_argument('-O','--OUT',help= "Path to Outfolder", required=False, default='./' )
parser.add_argument('-igb','--IGB',help="Path to IgBlast csv that should be UMI-corrected. IMPORTANT: Expects Columns: Locus, V, D, J, CDR3_aa, Spatial Barcode, UMI as generated by SPTCR-seq Pipeline",default="")
parser.add_argument('-c','--CUTOFF',help="Umi Corrected Count Cutoff for JSON Preparation", default=1 )
parser.add_argument('-outn','--OUTNAME',help="Extension to add to outfile", required=False,default="umi_corrected_TRAB_corr" )

args = parser.parse_args()

arg_vars = vars(args)
##################### Import Modules ############################
import sklearn.preprocessing as sk
import pandas as pd
import json
import os

### Disable Setting Value on Copy Warning
pd.options.mode.chained_assignment = None

#######################################################
#################### Variables ########################
OUT=str(arg_vars["OUT"])
OUTNAME=arg_vars["OUTNAME"]

#sample_name=arg_vars["NAME"]
read_dir=str(arg_vars["IGB"])
cutoff=arg_vars["CUTOFF"]


#######################################################
############ Preparing JSON for MADHYPE #############


## Read in IGB and Subset by Cutoff
igb=pd.read_csv(read_dir)

## Get Data, Give Well ID and split into TRA & TRB
igb['Well ID']=igb.groupby(['Spatial Barcode']).ngroup()

igb['TCR'] = igb[['Locus','V','D','J','CDR3_aa']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)
igb=igb[['Spatial Barcode','Uncorrected Count','UMI Corrected','Well ID','TCR']]

TRA=igb[igb['TCR'].str.contains('TRA_')]
TRA=TRA.rename(columns={'TCR':'TRA'})

TRB=igb[igb['TCR'].str.contains('TRB_')]
TRB=TRB.rename(columns={'TCR':'TRB'})

print(TRA,TRB)
del igb


### Groupby TCR Barcode Combination and Encode TCR as ID

TRA_count=TRA.sort_values(by='UMI Corrected',ascending=False).reset_index(drop=True)
TRA_count['TCR ID']=TRA_count.groupby(['TRA']).ngroup()

TRB_count=TRB.sort_values(by='UMI Corrected',ascending=False).reset_index(drop=True)
TRB_count['TCR ID']=TRB_count.groupby(['TRB']).ngroup()


## Scale TRA & TRB Count

TRB_count_tansformed=TRB_count
TRA_count_tansformed=TRA_count

if len(TRB_count['UMI Corrected']) > len(TRA_count['UMI Corrected']):
    #print('TRB')
    MAX=TRB_count['UMI Corrected'].max()
    MIN=TRB_count['UMI Corrected'].min()
else:
    #print('TRA')
    MAX=TRA_count['UMI Corrected'].max()
    MIN=TRA_count['UMI Corrected'].min()
#print(MAX,MIN)

################## Initiate Scaler##################

### Min Max Scaler
scaler=sk.MinMaxScaler(feature_range=(MIN, MAX))

## Prepare Data and Scale
Tranformed_TCR_count=pd.DataFrame()
Tranformed_TCR_count['TRA Unscaled UMI Corrected']=TRA_count['UMI Corrected']
Tranformed_TCR_count['TRB Unscaled UMI Corrected']=TRB_count['UMI Corrected']

Tranformed_TCR_count[['TRA Transformed','TRB Transformed']]=scaler.fit_transform(Tranformed_TCR_count[['TRA Unscaled UMI Corrected','TRB Unscaled UMI Corrected']])
Tranformed_TCR_count=Tranformed_TCR_count[(Tranformed_TCR_count['TRA Transformed'] > cutoff)&(Tranformed_TCR_count['TRB Transformed'] > cutoff)]
#Tranformed_TCR_count=Tranformed_TCR_count.dropna(subset=['TRA Transformed','TRB Transformed'])

Tranformed_TCR_count=Tranformed_TCR_count.round().astype(int)

print('Tranformed_TCR_count',Tranformed_TCR_count)


### Add Scaled Data to old count DF and cutoff at decided value

TRB_count['Count Transformed']=Tranformed_TCR_count['TRB Transformed'].round().astype(int)
TRB_count['Count Transformed']=TRB_count['Count Transformed'].fillna(0)
TRB_count['Count Transformed']=TRB_count['Count Transformed'].round().astype(int)
TRB_count=TRB_count[TRB_count['Count Transformed']>=cutoff]

TRA_count['Count Transformed']=Tranformed_TCR_count['TRA Transformed']
TRA_count['Count Transformed']=TRA_count['Count Transformed'].fillna(0)
TRA_count['Count Transformed']=TRA_count['Count Transformed'].round().astype(int)
TRA_count=TRA_count[TRA_count['Count Transformed']>=cutoff]

print('Scaled TRB',TRB_count)
print('Scaled TRA',TRA_count)



### Fill Well Data

### Encode Well TCR Combinations
TRB_Encoding=TRB_count.groupby('Well ID').agg({'TCR ID':lambda x: list(x)})

TRA_Encoding=TRA_count.groupby('Well ID').agg({'TCR ID':lambda x: list(x)})

## Filter to only get matching Well Combinations

TRB_Encoding_filtered=TRB_Encoding.filter(items=TRA_Encoding.index,axis=0)

TRA_Encoding_filtered=TRA_Encoding.filter(items=TRB_Encoding.index,axis=0)

### Add to Dict
well_data={}
well_data['A']=TRA_Encoding['TCR ID'].to_list()
well_data['B']=TRB_Encoding['TCR ID'].to_list()
#_filtered
#_filtered

###### Cells Per Well

TRB_count['Cell Sum']=TRB_count.groupby(['Spatial Barcode'])['Count Transformed'].transform('sum')
TRA_count['Cell Sum']=TRA_count.groupby(['Spatial Barcode'])['Count Transformed'].transform('sum')

TRB_per_well=TRB_count[['Well ID','Cell Sum']]
TRB_per_well=TRB_per_well.groupby('Well ID').mean()
TRB_per_well['Cell Sum']=TRB_per_well['Cell Sum'].astype(int)

TRA_per_well=TRA_count[['Well ID','Cell Sum']]
TRA_per_well=TRA_per_well.groupby('Well ID').mean()
TRA_per_well['Cell Sum']=TRA_per_well['Cell Sum'].astype(int)


## Merge TRA/TRB to one for cell sum
cell_sum=pd.merge(left=TRA_per_well,right=TRB_per_well, left_index=True, right_index=True)

## Use Mean to Generate Cell Count
cell_sum['MEAN TRA TRB']=cell_sum.mean(axis=1).astype(int)

mean_cell_count= cell_sum['MEAN TRA TRB'].mean()

cell_count= int(cell_sum['MEAN TRA TRB'].sum())


################ Write JSON #################################

#### Cells & Pairs
#simulated_data=json.load(open('/media/jkbuntu/SAMSUNG2TB/Dropbox/KBJasim/Projects/Capture_Sequencing/TRA_TRB_Full_TCR/8_6_22/WORK/simulated_data.json'))

### Write own cells and pairs into json
cells_TRB=TRB_count[['Well ID','TCR ID','Count Transformed']]
cells_TRB=cells_TRB.rename(columns={'TCR ID' :'TCR ID TRB','Count Transformed':'Count Transformed TRB'})

cells_TRA=TRA_count[['Well ID','TCR ID','Count Transformed']]
cells_TRA=cells_TRA.rename(columns={'TCR ID' :'TCR ID TRA','Count Transformed':'Count Transformed TRA'})

merged_cells=cells_TRA.merge(cells_TRB,on='Well ID')

Total_cells=merged_cells['Count Transformed TRA'].sum()+merged_cells['Count Transformed TRB'].sum()
merged_cells['Frequency']=merged_cells.apply(lambda x:(x['Count Transformed TRB']+x['Count Transformed TRA'])/Total_cells,axis=1)

merged_cells=merged_cells.sort_values(by='Frequency',ascending=False).reset_index(drop=True)

print(merged_cells)

dump_frame=merged_cells[['TCR ID TRA','TCR ID TRB','Frequency']]
dump_frame_dict=dump_frame.to_dict(orient='list')
cell_dump=[]
for zipped_tup in zip(dump_frame_dict['TCR ID TRA'],dump_frame_dict['TCR ID TRB'],dump_frame_dict['Frequency']):
    cell_dump.append([[[zipped_tup[0]],[zipped_tup[1]]],zipped_tup[2]])

data_dump={}
data_dump['cells']=cell_dump
data_dump['pairs']=cell_dump

#### Options
options={'compare': False,
'alpha_sharing_probs': 0.0,
'reference': None,
'cell_freq_constant': 2.0,
'chain_deletion_prob': 0.1,
'visual_block': True,
'ax': None,
'plot_frequency_estimation': False,
'neg_color': 'white',
'beta_sharing_probs': 0.0,
'plot_repertoire': False,
'silent': False,
'title': True,
'alpha_dual_prob': 0.0,
'pos_color': 'black',
'plot_comparison': True,
'fig': None,
'save': True,
'figsize': [6, 12],
'savename': 'img_{}.png',
'visual': True,
'num_cells': cell_count,
'hold': True,
'legend': True,
'cell_freq_distro': 'power-law',
'plot_auroc': False,
'beta_dual_prob': 0.0,
'fdr_plot': 0.01,
'max_pairs': 1000000,
'print': False,
'fdr': 0.01,
'chain_misplacement_prob': 0.0}

#,'cell_freq_max': 0.01,

### Build Tuple with the number of wells
well_count=len(cell_sum.index.unique())
options['num_wells']=tuple([1 for x in range(0,well_count)])

### Generate tuple for Mean Cell Count per Well
options['cpw']=tuple(cell_sum['MEAN TRA TRB'].to_list())

### Max Cell Frequency
max_freq=merged_cells['Frequency'].max()
options['cell_freq_max']=max_freq


#### Dump to JSON 
dump={}
dump['well_data']=well_data
dump['options']=options

dump['cells']=data_dump['cells']
dump['pairs']=data_dump['pairs']

with open("{0}.json".format(OUTNAME), "w") as outfile:
    json.dump(dump, outfile,  indent = 3)
